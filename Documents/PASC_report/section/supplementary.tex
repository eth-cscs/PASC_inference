\subsection{Principle of proof by contradiction}

Let us suppose, that we want to (have to) prove the implication
\begin{equation}
 \label{eq:contradiction_implication}
 v_1 \Rightarrow v_2,
\end{equation}
where $v_1, v_2$ are statements\footnote{statement is a meaningful declarative sentence that is either true or false}. Suppose that "standart" direct proof in form
\begin{displaymath}
 v_1 \Rightarrow ~~~ 
 \hat{v}_1 \Rightarrow
 \dots
 \Rightarrow \hat{v}_m
 ~~~ \Rightarrow v_2
\end{displaymath}
is not suitable or too complicated. Here $\hat{v}_1, \dots, \hat{v}_m$ denote auxiliary statements presenting the small partial steps during the proof.

At first, let us recall the \emph{Truth table} of the implication and other interesting relationship between statements (i.e. composed statements). 
In Table \ref{table:statements}, $1$ denotes that the statement is \emph{true}, $0$ denotes \emph{false}, and $v'$ is the negation of statement $v$.

\begin{table}[h]
%\def\arraystretch{1.3}
\centering
\begin{tabular}{C{0.05\linewidth} | C{0.05\linewidth} || C{0.1\linewidth} | C{0.1\linewidth} | C{0.1\linewidth} }
\hline
 $v_1$ & $v_2$ & $v_1 \Rightarrow v_2$ & $v_1' \vee v_2$ & $v_1 \wedge v_2'$ \\
\hline
 1 & 1 & 1 & 1 & 0 \\
 1 & 0 & 0 & 0 & 1 \\
 0 & 1 & 1 & 1 & 0 \\
 0 & 0 & 1 & 1 & 0 \\
\hline
\end{tabular}
\caption{Truth of selected composed statements; notice that 3th and 4th columns are equivalent, 5th is the negation of 3th and 4th column}
\label{table:statements}
\end{table}

Please, notice that from the table it is clear that statement $v_1 \Rightarrow v_2$ is equivalent to $v_1' \vee v_2$. However, instead of proving this statement (the first one
or the second one, it does not matter which one, since they are equivalent), we can prove that the negation  of this statement is not true
\footnote{because if the negation is not true, than the original statement is true}. The negation of implication is given by $v_1 \wedge v_2'$, see Table \ref{table:statements}.
To see the real relationship between $v_1' \vee v_2$ and $v_1 \wedge v_2'$, please, see also so-called \emph{De-Morgan's laws}.

Therefore, if we want to prove the implication \eqref{eq:contradiction_implication}, then we rather examine $v_1 \wedge v_2'$, i.e. we suppose, that the assumptions of the implication are true and the result in not true in the same time.
To prove that this statement is not true, it is enought to show that it does not hold in any case, i.e. we state the \emph{contradiction}.

Moreover, if we consider additional quantifiers (like \emph{for all} or \emph{there exists}) and our statements depend on the parameters, 
then during the negation of original implication statement, we have to perform negation also to all quantifiers.
For example the negation of
\begin{displaymath}
 \forall \alpha: v_1(\alpha) \Rightarrow v_2(\alpha)
\end{displaymath}
is given by
\begin{displaymath}
 \exists \alpha: v_1(\alpha) \wedge v_2'(\alpha).
\end{displaymath}
So, if we prove that this $\alpha$ does not exist, then we are done.



\subsection{Absolute values in constraints}

\begin{theorem}
Let
\begin{itemize}
 \item $f: \mathbb{R}^n \rightarrow \mathbb{R}, f \in C^1(\mathbb{R}^n)$ be a convex function,
 \item $c \in \mathbb{R}^{+}$ is arbitrary constant,
 \item $S \in \mathbb{R}^{n,2n}$ is a matrix defined by
 \begin{displaymath}
  S := 
  \left[
   \begin{array}{ccccccc}
     1 & -1 & & & & & \\
      &  & 1 & -1 & & & \\
      &  & &  & \ddots & & \\
      &  &  &  & & 1 & -1 
   \end{array}
  \right],
 \end{displaymath}
 \item $h(y) := f(Sy), h: \mathbb{R}^{2n} \rightarrow \mathbb{R}$.
\end{itemize}
Then if there exists a solution of optimization problem
\begin{equation}
 \label{eq:absvalconstr1}
 \bar{y} := \arg \min h(y) ~~~ \textrm{subject to} ~~~ \sum\limits_{i=1}^{2n} y_i \leq c ~~ \textrm{and} ~~ y \geq 0,
\end{equation}
then $\bar{x} := S \bar{y}$ is a solution of optimization problem
\begin{equation}
 \label{eq:absvalconstr2}
 \bar{x} = \arg \min f(x) ~~~ \textrm{subject to} ~~~ \Vert x \Vert_1 \leq c.
\end{equation}
Moreover
\begin{equation}
 \label{eq:absvalconstr3}
\forall i = 1,\dots,n: \bar{y}_{2i-1} \cdot \bar{y}_{2i} = 0.
\end{equation}
\end{theorem}

\begin{proof}
At first, notice that $h \in C^1(\mathbb{R}^{2n})$ is also a convex function and (here $\langle g(x),x=\hat{x} \rangle$ denotes function value of $g(x)$ in point $\hat{x}$)
\begin{equation}
  \label{eq:absvalconstr4}
  \langle \nabla h(y), y = \hat{y} \rangle = 
  \langle \nabla f(Sy), y = \hat{y} \rangle = 
  \langle S^T \nabla f(x), x = S \hat{y} \rangle.
\end{equation}
Since we suppose existence of $\bar{y}$ as a solution of \eqref{eq:absvalconstr1}, then from necessary optimality condition (see \todo{give cite Boyd, Vandenberghe: Convex Optimization, page 139, equation 4.21})
we get (here $\langle v,w \rangle$ denotes scalar product of vectors $v,w \in \mathbb{R}^m$)
\begin{equation}
 \label{eq:absvalconstr5}
  \langle \nabla h(\bar{y}), y - \bar{y} \rangle \geq 0, ~~~ \forall y \in \Omega_y,
\end{equation}
where $\Omega_y \subset \mathbb{R}^{2n}$ is a feasible set of \eqref{eq:absvalconstr1}. Using \eqref{eq:absvalconstr4}, we can write condition \eqref{eq:absvalconstr5}
in form
\begin{equation}
 \label{eq:absvalconstr6}
  \langle S^T \nabla f(S \bar{y}), y - \bar{y} \rangle \geq 0, ~~~ \forall y \in \Omega_y.
\end{equation}
Let us denote $\bar{x} := S \bar{y}$. Using the definition of scalar product
\begin{displaymath}
 \forall v,w \in \mathbb{R}^m: \langle v,w \rangle = v^T w = \sum\limits_{i = 1}^{m} v_i w_i,
\end{displaymath}
we can write the left part of \eqref{eq:absvalconstr6} in form
\begin{equation}
 \label{eq:absvalconstr7}
 \langle S^T \nabla f(\bar{x}), y - \bar{y} \rangle
 = \left( S^T \nabla f(\bar{x}) \right)^T \left( y - \bar{y} \right)
 = \left( \nabla f(\bar{x}) \right)^T S \left( y - \bar{y} \right)
 = \langle \nabla f(\bar{x}), Sy - \bar{x} \rangle.
\end{equation}
It remains to prove the relationship between $\Omega_x$ (feasible set of \eqref{eq:absvalconstr2}) and $\Omega_y$ (feasible set of \eqref{eq:absvalconstr1}) in form
\begin{equation}
 \label{eq:absvalconstr8}
 \begin{array}{ll}
  \forall y \in \mathbb{R}^{2n}: & y \in \Omega_y ~~ \Rightarrow ~~ Sy \in \Omega_x, \\
  \forall x \in \mathbb{R}^{n}: & x \in \Omega_x ~~ \Rightarrow ~~ \exists ! y \in \Omega_y: Sy = x, 
 \end{array}
\end{equation}
because if \eqref{eq:absvalconstr8} holds, then we are able to write \eqref{eq:absvalconstr6} using \eqref{eq:absvalconstr7} in form
\begin{displaymath}
 \forall x \in \Omega_x: \langle \nabla f(\bar{x}), x - \bar{x} \rangle \geq 0,
\end{displaymath}
that concludes that $\bar{x} \in \Omega_x$ is solution of \eqref{eq:absvalconstr5}.\newline

\vspace{0.5cm}

\noindent To prove the first statement of \eqref{eq:absvalconstr8}, let us consider $y \in \Omega_y$, i.e. $y \in \mathbb{R}^{2n}$ such that
\begin{equation}
 \label{eq:absvalconstr9}
 \sum\limits_{i = 1}^{2n} y_i \leq c, y \geq 0.
\end{equation}
In following, we examine the L1-norm of $x := Sy$
\begin{equation}
 \label{eq:absvalconstrst}
 \Vert x \Vert_1 = \Vert Sy \Vert_1 = \Vert [y_2 - y_1, y_4 - y_3, \dots, y_{2n} - y_{2n-1}]^T \Vert_1 = \sum\limits_{i =1}^n \vert y_{2i} - y_{2i-1} \vert
\end{equation}
using auxiliary inequality; please, notice that
\begin{displaymath}
 \forall \alpha, \beta \in \mathbb{R}: \vert \alpha - \beta \vert \leq \vert \alpha \vert + \vert \beta \vert,
\end{displaymath}
therefore we can estimate \eqref{eq:absvalconstrst}
\begin{displaymath}
 \sum\limits_{i = 1}^{n} \vert y_{2i} - y_{2i-1} \vert \leq \sum\limits_{i=1}^n \left( \vert y_{2i} \vert + \vert y_{2i-1} \vert \right) = \sum\limits_{i=1}^{2n} \vert y_i \vert .
\end{displaymath}
Since we assume that $y \in \Omega_y$, i.e. \eqref{eq:absvalconstr9}, we can continue with estimation
\begin{displaymath}
 \sum\limits_{i=1}^{2n} \vert y_i \vert = \sum\limits_{i=1}^{2n} y_i  \leq c,
\end{displaymath}
i.e. $\Vert x \Vert_1 \leq c$ and therefore $x \in \Omega_x$. \newline

\vspace{0.5cm}

\noindent Now we prove the second statement of \eqref{eq:absvalconstr8}. Suppose $x \in \Omega_x$, i.e.
\begin{equation}
 \label{eq:absvalconstr12}
 \Vert x \Vert_1 = \sum\limits_{i=1}^n \vert x_i \vert \leq c.
\end{equation}
Please, notice that for any $\alpha \in \mathbb{R}$, there exist $\alpha^{+}, \alpha^{-} \geq 0$ such that
\begin{equation}
 \label{eq:absvalconstr10}
 \begin{array}{rcl}
  \alpha & = & \alpha^{+} - \alpha^{-}, \\
  \vert \alpha \vert & = & \alpha^{+} + \alpha^{-},
 \end{array}
\end{equation}
(for instance
if $\alpha \geq 0$, then we can choose $\alpha^{+}:=\alpha, \alpha^{-}:=0$;
if $\alpha < 0$, then we can choose $\alpha^{+}:=0, \alpha^{-}:= -\alpha$ )
\footnote{in fact, in the end of the proof it will be clear that this decomposition is only one possible choice}.
Using \eqref{eq:absvalconstr10}, we can decompose each component $x_i, i=1,\dots,n$ into
\begin{equation}
 \label{eq:absvalconstr11}
 \begin{array}{rcl}
  x_i & = & y_{2i-1} - y_{2i}, \\
  \vert x_i \vert & = & y_{2i-1} + y_{2i},
 \end{array}
\end{equation}
where $y \in \mathbb{R}^{2n}, y \geq 0$ is a new vector.
We examine the first condition of \eqref{eq:absvalconstr9} using the second equality of \eqref{eq:absvalconstr11} 
and assumption \eqref{eq:absvalconstr12}
\begin{displaymath}
 \sum\limits_{i=1}^{2n} y_i = \sum\limits_{i =1}^{n} y_{2i-1} + y_{2i} = \sum\limits_{i=1}^{n} \vert x_i \vert \leq c.
\end{displaymath}
Therefore, we can state that $y \in \Omega_y$. Moreover, notice that from the first equality in \eqref{eq:absvalconstr11}, it holds $x = Sy$.

\vspace{0.5cm}

\noindent To prove \eqref{eq:absvalconstr3}, it is sufficient to show that \eqref{eq:absvalconstr10} gives us $\alpha^{+} \cdot \alpha^{-} = 0$
\footnote{notice that we do not need assumption $\alpha^{+},\alpha^{-} \geq 0$}, 
i.e.
\begin{equation}
 \label{eq:absvalconstr17}
 \left.
 \begin{array}{rcl}
  \alpha & = & \alpha^{+} - \alpha^{-} \\
  \vert \alpha \vert & = & \alpha^{+} + \alpha^{-}
 \end{array}
 \right\rbrace ~~~ \Rightarrow
 ~~~ \alpha^{+} \cdot \alpha^{-} = 0.
\end{equation}
Suppose by contradiction that $\alpha^{+} \cdot \alpha^{-} \neq 0$, i.e.
\begin{equation}
 \label{eq:absvalconstr13}
 \alpha^{+} \neq 0 ~~~~ \textrm{and} ~~~~ \alpha^{-} \neq 0.
\end{equation}
Now we get rid of absolute value in system in assumption of \eqref{eq:absvalconstr17}.
We examine both of possible situations:
\begin{itemize}
 \item if $\alpha \geq 0$, then the solution of system
  \begin{displaymath}
   \begin{array}{rcl}
    \alpha & = & \alpha^{+} - \alpha^{-}, \\
    \alpha& = & \alpha^{+} + \alpha^{-}, 
   \end{array}
  \end{displaymath}
  is given by $\alpha^{+} = \alpha, \alpha^{-} = 0$, which is a contradition with \eqref{eq:absvalconstr13}.
 \item if $\alpha < 0$, then the solution of system
  \begin{displaymath}
   \begin{array}{rcl}
    \alpha & = & \alpha^{+} - \alpha^{-}, \\
    -\alpha& = & \alpha^{+} + \alpha^{-}, 
   \end{array}
  \end{displaymath}
  is given by $\alpha^{+} = 0, \alpha^{-} = -\alpha$, which is a contradition with \eqref{eq:absvalconstr13}.
\end{itemize}

We proved that the solution of the system \eqref{eq:absvalconstr11} is unique, therefore for each $x \in \Omega_x$ there exists unique $y \in \Omega_y$ and $x = Sy$.

\end{proof}

\begin{example}
Let us consider the basic quadratic programming problem - the projection (Euclidean) onto feasible set. In this case, we will consider a feasible set described by the L1-norm.
On this simple example, we demonstrate the computation of gradient in equation \eqref{eq:absvalconstr4}.
For any $p \in \mathbb{R}^2$, we define projection onto set $\Omega$ as a solution of optimization problem
\begin{displaymath}
 P_{\Omega} (p) = \arg \min\limits_{x \in \Omega} \Vert x - p \Vert, ~~~ \Omega := \lbrace x \in \mathbb{R}^2: \Vert x \Vert_1 \leq 1 \rbrace
\end{displaymath}
(find the point from $\Omega$ which is the nearest to given arbitrary $p$).
Notice that the problem is equivalent to
\begin{displaymath}
 P_{\Omega} (y) = \arg \min\limits_{x \in \Omega} \frac{1}{2}\Vert x - p \Vert^2 = \arg \min\limits_{x \in \Omega} \underbrace{\frac{1}{2} x^T I x - p^T x}_{=: f(x)}
\end{displaymath}
with gradient
\begin{displaymath}
 \nabla f(x) = x - p.
\end{displaymath}
Let us define the function $h: \mathbb{R}^{4n} \rightarrow \mathbb{R}$ by
\begin{displaymath}
 h(y) := f(Sy) = \frac{1}{2} (Sy)^T I Sy - p^T Sy = \frac{1}{2} y^T S^T S y - (S^Tp)^T y.
\end{displaymath}
The gradient of this function is given by
\begin{displaymath}
 \nabla h(y) = S^T S y - S^Tp = S^T ( Sy-p) = S^T \nabla f(Sy).
\end{displaymath}
Instead of solving the original problem, we will rather solve
\begin{displaymath}
 \hat{y} = \arg \min\limits_{y \in \Omega_y} \frac{1}{2} y^T S^T S y - (S^Tp)^T y, ~~~ \Omega_y := \lbrace y \in \mathbb{R}^4: y_1 + y_2 + y_3 + y_4 \leq c ~ \vee ~ y \geq 0 \rbrace
\end{displaymath}
and the solution of original problem will be given by $P(p) = S\hat{y}$. Notice that $S^TS$ is SPS Hessian matrix of $h$, therefore $h$ is convex function (but not strictly convex).
\end{example}

\subsection{QP solvability}


