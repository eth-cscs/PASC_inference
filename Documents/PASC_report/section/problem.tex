 In our project, we analyze the time-series, i.e. the sequence of given data
 \begin{equation}
  \label{eq:timeseries}
	x_0, x_1, \dots, x_{T-1},
 \end{equation}
 where $x_t \in \mathbb{R}^{\mathrm{xdim}}$ and $\mathrm{xdim} \in \mathbb{N}$ is the number of values (measurements) in each time step. 
 We try to understand the inner mechanism (dynamics) of the sequence. \newline
 
 As the first step of analyze of given data, we can use classical tools and easily compute "static"\footnote{I decided to use this term, because following properties does not take into account the time} statistics, like average, variance, deviation and other moments or central moments.
 These values provide us the basic properties of the given set of data, however they does not take into account the fact, that we are not working with only set, but we analyze the sequence.

 One of the most typical way how to analyze sequences is to appoximate the given data by much more simplier function. Afterwards, analyzing this approximating function provides us the basic knowledge of sequence behaviour with respect to time. 
 The approximation is performed to be "as good as possible", i.e. in a such way, that the error of the approximation is as small as possible.
 
 The most simplest approximation function is linear\footnote{in fact, the simplest approximation function is constant function; however the constant function is in this case the average of given set of data; constant function is constant with respect to time and therefore it cannot reflect the dynamics of the sequence}.
 In this case, we are talking about \emph{linear regression}.
 
 For the simplicity, in the following we will suppose one dimensional data $\mathrm{xmem} = 1$, i.e. $x(t) \in \mathbb{R}$. 
 Provided observations could be easily generalized to more dimensions.
 
 Suppose that all given data \eqref{eq:timeseries} linearly depends on the time, i.e. each data point could be written in form $x(t) = \alpha_0 + \alpha_1 t$, where $\alpha_0,\alpha_1 \in \mathbb{R}$ are unknown time-independent parameters of this linear \emph{model}. 
 However, general sequence is not generated by linear function, therefore in each time-step we make an error. To be more exact, we rather write
 \begin{equation}
  \label{eq:linear}
  x(t) = \alpha_0 + \alpha_1 t + \varepsilon_t, ~~t = 0,\dots,T,
 \end{equation}
 where $\varepsilon_t$ is an error of approximation (hopefully small for all $t$). 
 If we denote 
 \begin{displaymath}
  \begin{array}{rcl}
   x & = & [x_0, \dots, x_{T-1}]^T \in \mathbb{R}^T, \\[5mm]
   Z & = & \left[
    \begin{array}{ccccc}
     1 & 1 & 1 & \dots & 1 \\
     0 & 1 & 2 & \dots & T-1
    \end{array} 
     \right] \in \mathbb{R}^{2,T}, \\[5mm]
   y & = & [\alpha_0,\alpha_1]^T \in \mathbb{R}^2, \\[5mm]
   \varepsilon & = & [\varepsilon_0,\dots,\varepsilon_{T-1}] \in \mathbb{R}^T,
  \end{array}
 \end{displaymath}
 then the system of equations \eqref{eq:linear} could be written in form
 \begin{equation}
  \label{eq:linear2}
  x = Z^T y + \epsilon.
 \end{equation}
 We want to perform the approximation (i.e. find $y$) in the best way as possible. We minimize the size of the error $\Vert \varepsilon \Vert \rightarrow \min\limits_{y}$. 
 Using \eqref{eq:linear2}, we can substitute and we get optimization problem
 \begin{equation}
  \label{eq:linear_opt}
  \hat{y} = \arg\min\limits_{y} \Vert \varepsilon \Vert  = \arg\min\limits_{y} \Vert Z^T y - x \Vert = \arg\min\limits_{y} \underbrace{\frac{1}{2}\Vert Z^T y - x \Vert^2}_{ = \Psi(y)}  
 \end{equation}
 Please, notice that $\Psi(y)$ is quadratic function
 \begin{displaymath}
  \Psi(y) = \frac{1}{2}\Vert Z^T y - x \Vert^2 = \frac{1}{2}\langle Z^T y - x, Z^T y - x \rangle = \frac{1}{2}y^T Z Z^T y - y^T Z x + \frac{1}{2}x^T x,
 \end{displaymath}
 therefore the gradient is given by
 \begin{displaymath}
  \nabla \Psi(y) = ZZ^T y - Z x
 \end{displaymath}
 and the necessary optimality condition of \eqref{eq:linear_opt} is given by the system of linear equations\footnote{yes, this is a least-square solution of our first naiive approach $x(t) = \alpha_0 + \alpha_1 t$}
 \begin{displaymath}
  ZZ^T y = Zx .
 \end{displaymath}

% \begin{example}
%  Let us consider data
%  \begin{displaymath}
%   x_0 = 
%  \end{displaymath}
% \end{example}

 Using a simple generalization idea, we are able to extend the linear regression model to polynomial models
 \begin{displaymath}
  x(t) = p(t) + \varepsilon_t, p \in \mathcal{P}_n,
 \end{displaymath}
 where $\mathcal{P}_n$ is a vector space of polynomial function of degree $n \in \mathbb{N}$.
